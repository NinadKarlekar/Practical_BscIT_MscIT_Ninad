{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fURktcnSY_w"
      },
      "source": [
        "# 4. Text Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X29Jjdz0W38k"
      },
      "source": [
        "## a. Tokenization using Python’s split() function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YN6cUfkBPmxh",
        "outputId": "572cfac2-3a62-43c6-a841-8b9d05ad946d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "4A. Tokenization using Python’s split() function\n",
            "----------\n",
            "Tokens: ['Hello', '!', 'My', 'name', 'is', 'Ninad', 'Karlekar', 'I', 'live', 'in', 'mumbai']\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# 4A. Tokenization using Python’s split() function\n",
        "\n",
        "# Sample text to tokenize\n",
        "text = \"Hello ! My name is Ninad Karlekar I live in mumbai\"\n",
        "\n",
        "# Tokenizing the text using split()\n",
        "tokens = text.split()\n",
        "\n",
        "# Printing the tokens\n",
        "print(\"=\"*60)\n",
        "print(\"4A. Tokenization using Python’s split() function\")\n",
        "print(\"-\"*10)\n",
        "print(\"Tokens:\", tokens)\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yl19YuRwWtFS"
      },
      "source": [
        "## b. Tokenization using Regular Expressions (RegEx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yN73RvfEQ-4X",
        "outputId": "f36af3a1-bad0-49a1-d5a4-665da2317002"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "4b. Tokenization using Regular Expressions (RegEx)\n",
            "----------\n",
            "Tokens: ['Hello', '!', 'My', 'name', 'is', 'Ninad', 'Karlekar', 'I', 'live', 'in', 'mumbai']\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# 4b. Tokenization using Regular Expressions (RegEx)\n",
        "\n",
        "import re\n",
        "\n",
        "# Sample text to tokenize\n",
        "text = \"Hello ! My name is Ninad Karlekar I live in mumbai\"\n",
        "\n",
        "# Define the regex pattern for tokenization (splitting by whitespace)\n",
        "pattern = r'\\s+'\n",
        "\n",
        "# Tokenizing the text using re.split()\n",
        "tokens = re.split(pattern, text)\n",
        "\n",
        "# Printing the tokens\n",
        "print(\"=\"*60)\n",
        "print(\"4b. Tokenization using Regular Expressions (RegEx)\")\n",
        "print(\"-\"*10)\n",
        "print(\"Tokens:\", tokens)\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQW3Dqr0Wqbq"
      },
      "source": [
        "## c. Tokenization using NLTK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ayXR1GORfSm",
        "outputId": "33a726b9-8765-4140-c785-7eb82de3398f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "4c. Tokenization using NLTK\n",
            "----------\n",
            "Tokens: ['Hello', '!', 'My', 'name', 'is', 'Ninad', 'Karlekar', 'I', 'live', 'in', 'mumbai']\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "#4c. Tokenization using NLTK\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Sample text to tokenize\n",
        "text = \"Hello ! My name is Ninad Karlekar I live in mumbai\"\n",
        "\n",
        "# Tokenizing the text using NLTK's word_tokenize()\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# Printing the tokens\n",
        "print(\"=\"*60)\n",
        "print(\"4c. Tokenization using NLTK\")\n",
        "print(\"-\"*10)\n",
        "print(\"Tokens:\", tokens)\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DAk8KVcWmJ0"
      },
      "source": [
        "## d. Tokenization using the spaCy library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_TD_NP-oVohB",
        "outputId": "e91759ad-8c56-40e8-f8cc-67db1d54cc17"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "4d. Tokenization using the spaCy library\n",
            "----------\n",
            "Tokens: ['Hello', '!', 'My', 'name', 'is', 'Ninad', 'Karlekar', 'I', 'live', 'in', 'mumbai']\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "#4d. Tokenization using the spaCy library\n",
        "\n",
        "import spacy\n",
        "\n",
        "# Load the English language model\n",
        "nlp = spacy.blank(\"en\")\n",
        "\n",
        "# Text to be tokenized\n",
        "text = \"Hello ! My name is Ninad Karlekar I live in mumbai\"\n",
        "\n",
        "# Process the text with SpaCy\n",
        "doc = nlp(text)\n",
        "\n",
        "# Extract tokens\n",
        "tokens = [token.text for token in doc]\n",
        "\n",
        "# Print tokens\n",
        "print(\"=\"*60)\n",
        "print(\"4d. Tokenization using the spaCy library\")\n",
        "print(\"-\"*10)\n",
        "print(\"Tokens:\", tokens)\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZE6xe3zWiLu"
      },
      "source": [
        "## e. Tokenization using Keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1NkLfrXqV0hW",
        "outputId": "ff6a86da-e743-4496-bc96-36509c0a5ac3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "4e. Tokenization using Keras\n",
            "----------\n",
            "Tokens: ['hello', 'my', 'name', 'is', 'ninad', 'karlekar', 'i', 'live', 'in', 'mumbai']\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "#4e. Tokenization using Keras [COLAB]\n",
        "\n",
        "import keras\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import text_to_word_sequence #works on colab\n",
        "\n",
        "# Create a string input\n",
        "str = \"Hello ! My name is Ninad Karlekar I live in mumbai\"\n",
        "\n",
        "# tokenizing the text\n",
        "tokens = text_to_word_sequence(str)\n",
        "print(\"=\"*60)\n",
        "print(\"4e. Tokenization using Keras\")\n",
        "print(\"-\"*10)\n",
        "print(\"Tokens:\", tokens)\n",
        "print(\"=\"*60)\n",
        "\n",
        "####\n",
        "# to run on local IDE(jupyter) use tenserflow version 2.13.0\n",
        "# pip uninstall tensorflow     ## to uninstall latest version if installed\n",
        "# pip install tensorflow==2.13.0\n",
        "####"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BMlVAOsWbGm"
      },
      "source": [
        "## f. Tokenization using Gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V04t0fhTV-Ez",
        "outputId": "79e8d815-7b93-448c-8f6a-9566af1a9e63"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "4f. Tokenization using Gensim\n",
            "----------\n",
            "Tokens: ['Hello', 'My', 'name', 'is', 'Ninad', 'Karlekar', 'I', 'live', 'in', 'mumbai']\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# 4f. Tokenization using Gensim\n",
        "#pip install gensim\n",
        "from gensim.utils import tokenize\n",
        "# Create a string input\n",
        "str = \"Hello ! My name is Ninad Karlekar I live in mumbai\"\n",
        "# tokenizing the text\n",
        "# Tokenizing the text\n",
        "tokenized_words = list(tokenize(str))\n",
        "\n",
        "# Printing each tokenized word separately\n",
        "print(\"=\"*60)\n",
        "print(\"4f. Tokenization using Gensim\")\n",
        "print(\"-\"*10)\n",
        "print(\"Tokens:\", tokenized_words)\n",
        "print(\"=\"*60)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
