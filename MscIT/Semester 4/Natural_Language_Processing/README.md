# Natural Language Processing

M. Sc (Information Technology)
Natural Language Processing



## Index

| Sr.No. | Name | README |
| --- | --- | --- |
| [Prac1A](/MscIT/Semester%204/Natural_Language_Processing/Practical01/) <br> [Prac1B](/MscIT/Semester%204/Natural_Language_Processing/Practical01/) | 1A. Convert file Text to Speech. <br> 1B. Convert file Speech to Text. | [Prac1A](#prac1) <br> [Prac1B](#prac1) |
| [Prac2A](/MscIT/Semester%204/Natural_Language_Processing/Practical02/) <br> [Prac2B](/MscIT/Semester%204/Natural_Language_Processing/Practical02/) <br> [Prac2C](/MscIT/Semester%204/Natural_Language_Processing/Practical02/) <br> [Prac2D](/MscIT/Semester%204/Natural_Language_Processing/Practical02/) <br> [Prac2E](/MscIT/Semester%204/Natural_Language_Processing/Practical02/) <br> [Prac2F](/MscIT/Semester%204/Natural_Language_Processing/Practical02/) <br> [Prac2G](/MscIT/Semester%204/Natural_Language_Processing/Practical02/) <br> [Prac2H](/MscIT/Semester%204/Natural_Language_Processing/Practical02/) | a. Study of various corpus – Brown, Inaugural, Reuters, udhr with various methods like fields, raw, words, sents, categories. <br> b. Create and use your own corpora (plaintext, categorical). <br> c. Study conditional frequency distribution. <br> d. Study of tagged corpora with methods like tagged_sents, tagged_words. <br> e. Write a program to find the most frequent noun tags. <br> f. Map words to the properties using Python Dictionaries. <br> g. Study i) DefaultTagger, ii) Regular expression tagger, iii) UnigramTagger <br> h. Find different words from a given plaintext without any spaces by comparing this text with a given corpus of words. Also find the score of words. | [Prac2A](#prac2) <br> [Prac2B](#prac2) <br> [Prac2C](#prac2) <br> [Prac2D](#prac2) <br> [Prac2E](#prac2) <br> [Prac2F](#prac2) <br> [Prac2G](#prac2) <br> [Prac2H](#prac2) |
| [Prac3A](/MscIT/Semester%204/Natural_Language_Processing/Practical03/) <br> [Prac3B](/MscIT/Semester%204/Natural_Language_Processing/Practical03/) <br> [Prac3C](/MscIT/Semester%204/Natural_Language_Processing/Practical03/) <br> [Prac3D](/MscIT/Semester%204/Natural_Language_Processing/Practical03/) <br> [Prac3E](/MscIT/Semester%204/Natural_Language_Processing/Practical03/) | a. Study of Wordnet Dictionary with methods as synsets, definitions, examples, antonyms. <br> b. Study lemmas, hyponyms, hypernyms, entailments. <br> c. Write a program using python to find synonym and antonym of word "active" using Wordnet. <br> d. Compare two nouns. <br> e. Handling stopword: <br> i. Using nltk, add or remove stop words in NLTK's Default stop word list <br> ii. Using Gensim, add or remove stop words in Default Gensim stop words List. <br> iii. Using SpaCy, add or remove Stop Words in Default SpaCy stop words List. | [Prac3A](#prac3) <br> [Prac3B](#prac3) <br> [Prac3C](#prac3) <br> [Prac3D](#prac3) <br> [Prac3E](#prac3) |
| [Prac4A](/MscIT/Semester%204/Natural_Language_Processing/Practical04/) <br> [Prac4B](/MscIT/Semester%204/Natural_Language_Processing/Practical04/) <br> [Prac4C](/MscIT/Semester%204/Natural_Language_Processing/Practical04/) <br> [Prac4D](/MscIT/Semester%204/Natural_Language_Processing/Practical04/) <br> [Prac4E](/MscIT/Semester%204/Natural_Language_Processing/Practical04/) <br> [Prac4F](/MscIT/Semester%204/Natural_Language_Processing/Practical04/) | Text Tokenization: <br> a. Tokenization using Python’s split() function. <br> b. Tokenization using Regular Expression (RegEx). <br> c. Tokenization using NLTK. <br> d. Tokenization using spaCy library. <br> e. Tokenization using Keras. <br> f. Tokenization using Gensim. | [Prac4A](#prac4) <br> [Prac4B](#prac4) <br> [Prac4C](#prac4) <br> [Prac4D](#prac4) <br> [Prac4E](#prac4) <br> [Prac4F](#prac4) |
| [Prac5A](/MscIT/Semester%204/Natural_Language_Processing/Practical05/) <br> [Prac5B](/MscIT/Semester%204/Natural_Language_Processing/Practical05/) <br> [Prac5C](/MscIT/Semester%204/Natural_Language_Processing/Practical05/) | Study the important libraries for Indian language and perform the following: <br> a. Word tokenization in Hindi <br> b. Generate similar sentences from a given Hindi text input <br> c. Identify the Indian language from the given text. | [Prac5A](#prac5) <br> [Prac5B](#prac5) <br> [Prac5C](#prac5) |
| [Prac6A](/MscIT/Semester%204/Natural_Language_Processing/Practical06/) <br> [Prac6B](/MscIT/Semester%204/Natural_Language_Processing/Practical06/) <br> [Prac6C](/MscIT/Semester%204/Natural_Language_Processing/Practical06/) | Illustrate Part-of-Speech: <br> a. Part of speech Tagging and chunking of user defined text. <br> b. Named Entity recognition of user defined text. <br> c. Named Entity recognition with diagram using NLTK corpus - treebank. | [Prac6A](#prac6) <br> [Prac6B](#prac6) <br> [Prac6C](#prac6) |
| [Prac7A](/MscIT/Semester%204/Natural_Language_Processing/Practical07/) <br> [Prac7B](/MscIT/Semester%204/Natural_Language_Processing/Practical07/) <br> [Prac7C](/MscIT/Semester%204/Natural_Language_Processing/Practical07/) <br> [Prac7D](/MscIT/Semester%204/Natural_Language_Processing/Practical07/) | a. Define grammar using nltk. Analyse a sentence using the same. <br> b. Accept the input string with Regular expression of FA: 101+ <br> c. Accept the input string with Regular expression of FA: (a+b)*bba <br> d. Implementation of Deductive Chart Parsing using context free grammar and a given sentence. | [Prac7A](#prac7) <br> [Prac7B](#prac7) <br> [Prac7C](#prac7) <br> [Prac7D](#prac7) |
| [Prac8A](/MscIT/Semester%204/Natural_Language_Processing/Practical08/) <br> [Prac8B](/MscIT/Semester%204/Natural_Language_Processing/Practical08/) | a. Study PorterStemmer, LancasterStemmer, RegexpStemmer, SnowballStemmer <br> b. Study WordNet Lemmatizer | [Prac8A](#prac8) <br> [Prac8B](#prac8) |
| [Prac9A](/MscIT/Semester%204/Natural_Language_Processing/Practical09/) | Implement Naive Bayes classifier | [Prac9A](#prac9) |
| [Prac10A](/MscIT/Semester%204/Natural_Language_Processing/Practical10/) <br> [Prac10B](/MscIT/Semester%204/Natural_Language_Processing/Practical10/) <br> [Prac10C](/MscIT/Semester%204/Natural_Language_Processing/Practical10/) <br> [Prac10D](/MscIT/Semester%204/Natural_Language_Processing/Practical10/) <br> [Prac10E](/MscIT/Semester%204/Natural_Language_Processing/Practical10/) | a. Speech Tagging: <br> i. Speech tagging using spacy. <br> ii. Speech tagging using NLTK. <br> b. Statistical parsing: <br> i. Usage of Give and Gave in the Penn Treebank sample. <br> ii. Probabilistic parser <br> c. Malt parsing: <br> i. Parse a sentence and draw a tree using malt parsing | [Prac10A](#prac10) <br> [Prac10B](#prac10) <br> [Prac10C](#prac10) <br> [Prac10D](#prac10) <br> [Prac10E](#prac10) |
| [Prac11A](/MscIT/Semester%204/Natural_Language_Processing/Practical11/) <br> [Prac11B](/MscIT/Semester%204/Natural_Language_Processing/Practical11/) <br> [Prac11C](/MscIT/Semester%204/Natural_Language_Processing/Practical11/) | a. Multiword Expressions in NLP <br> b. Normalized Web Distance and Word Similarity <br> c. Word Sense Disambiguation | [Prac11A](#prac11) <br> [Prac11B](#prac11) <br> [Prac11C](#prac11) |


























******************
---------------------

## Prac1

1A. Convert file Text to Speech.


<details>
<summary>CODE</summary>

```python
# Import the required module for text to speech conversion

#!pip install gtts
from gtts import gTTS

# This module is imported so that we can play the converted audio
import os

# The text that you want to convert to audio
mytext = "Hello Everyone!My name is Ninad"

# Language in which you want to convert
language = "en"

# Passing the text and language to the engine, here we have marked slow=False. Which tells the module that the converted audio should have a high speed
myobj = gTTS(text=mytext, lang=language, slow=False)

# Saving the converted audio in a mp3 file named welcome
myobj.save("welcome1.mp3")

# Playing the converted file
#os.system("mpg321 welcome.mp3")

```

</details>

<br>


1B. Convert file Speech to Text.


<details>
<summary>CODE</summary>


```python
#Aim: Convert audio file Speech to Text.
#Note: required to store the input file "NLP_test.wav" in the current folder before running the program.

#!pip install SpeechRecognition pydub
import speech_recognition as sr
filename = "MscIT\\Semester 4\\Natural_Language_Processing\\Practical01\\NLP_test.wav"

# initialize the recognizer
r = sr.Recognizer()
# open the file
with sr.AudioFile(filename) as source:
    # listen for the data (load audio to memory)
    audio_data = r.record(source)
    # recognize (convert from speech to text)
    text = r.recognize_google(audio_data)
    print(text)

```

</details>

******************************************************

## Prac2

2a. Study of various corpus – Brown, Inaugural, Reuters, udhr with various methods like fields, raw, words, sents, categories.

<details>
<summary>CODE</summary>

```python
"""## 2A. Study of various corpus – Brown, Inaugural, Reuters, udhr with various methods like fields, raw, words, sents, categories."""

import nltk
from nltk.corpus import brown

nltk.download('brown')

# Display file ids of brown corpus
print('File ids of brown corpus\n', brown.fileids())

# Pick out the first of these texts — Emma by Jane Austen — and give it a short name, ca01
ca01 = brown.words('ca01')
# Display first few words
print('\nca01 has the following words:\n', ca01[:20])
# Total number of words in ca01
print('\nca01 has', len(ca01), 'words')

# Categories or files in brown corpus
print('\n\nCategories or files in brown corpus:\n')
print(brown.categories())

# Display other information about each text by looping over all the values of fileid
# and then computing statistics for each text.
print('\n\nStatistics for each text:\n')
print('AvgWordLen\tAvgSentenceLen\tNo. of Times Each Word Appears On Avg\tFileName')

for fileid in brown.fileids():
    num_chars = len(brown.raw(fileid))
    num_words = len(brown.words(fileid))
    num_sents = len(brown.sents(fileid))
    num_vocab = len(set(w.lower() for w in brown.words(fileid)))

    print(f"{int(num_chars / num_words)}\t\t\t"
          f"{int(num_words / num_sents)}\t\t\t"
          f"{int(num_words / num_vocab)}\t\t\t"
          f"{fileid}")

```

</details>

<br>

2b. Create and use your own corpora (plaintext, categorical).

<details>
<summary>CODE</summary>

```python
## 2B. Create and use your own corpora (plaintext, categorical).

import os
import nltk
nltk.download('punkt')
from nltk.corpus import PlaintextCorpusReader

# Set the path to your corpus
corpus_root = '/content/uni' #path of files
filelist = PlaintextCorpusReader(corpus_root, '.*')

# Display file list
print('\nFile list:\n')
print(filelist.fileids())
print(filelist.root)

# Display other information about each text by looping over all the values of fileid
# and then computing statistics for each text.
print('\n\nStatistics for each text:\n')
print('AvgWordLen\tAvgSentenceLen\tNo. of Times Each Word Appears On Avg\tFileName')

for fileid in filelist.fileids():
    num_chars = len(filelist.raw(fileid))
    num_words = len(filelist.words(fileid))
    num_sents = len(filelist.sents(fileid))
    num_vocab = len(set(w.lower() for w in filelist.words(fileid)))

    print(f"{int(num_chars / num_words)}\t\t\t"
          f"{int(num_words / num_sents)}\t\t\t"
          f"{int(num_words / num_vocab)}\t\t"
          f"{fileid}")
```

</details>

<br>

2c. Study conditional frequency distribution.

<details>
<summary>CODE</summary>

```python
## 2c. Study Conditional frequency distributions

# Process a sequence of pairs
text = ['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]
pairs = [('news', 'The'), ('news', 'Fulton'), ('news', 'County'), ...]

import nltk
from nltk.corpus import brown
nltk.download('inaugural')
nltk.download('udhr')

fd = nltk.ConditionalFreqDist(
    (genre, word)
    for genre in brown.categories()
    for word in brown.words(categories=genre)
)

genre_word = [
    (genre, word)
    for genre in ['news', 'romance']
    for word in brown.words(categories=genre)
]

print(len(genre_word))
print(genre_word[:4])
print(genre_word[-4:])

cfd = nltk.ConditionalFreqDist(genre_word)

print(cfd)
print(cfd.conditions())
print(cfd['news'])
print(cfd['romance'])
print(list(cfd['romance']))

from nltk.corpus import inaugural

cfd = nltk.ConditionalFreqDist(
    (target, fileid[:4])
    for fileid in inaugural.fileids()
    for w in inaugural.words(fileid)
    for target in ['america', 'citizen']
    if w.lower().startswith(target)
)

from nltk.corpus import udhr

languages = [
    'Chickasaw', 'English', 'German_Deutsch',
    'Greenlandic_Inuktikut', 'Hungarian_Magyar', 'Ibibio_Efik'
]

cfd = nltk.ConditionalFreqDist(
    (lang, len(word))
    for lang in languages
    for word in udhr.words(lang + '-Latin1')
)

cfd.tabulate(conditions=['English', 'German_Deutsch'], samples=range(10), cumulative=True)
```

</details>

<br>

2d. Study of tagged corpora with methods like tagged_sents, tagged_words.

<details>
<summary>CODE</summary>

```python
## 2d. Study of tagged corpora with methods like tagged_sents, tagged_words.
import nltk
from nltk import tokenize

nltk.download('punkt')
nltk.download('words')

para = "Hello! My name is Ninad Karlekar. Today you'll be learning NLTK."
sents = tokenize.sent_tokenize(para)

print("\nSentence tokenization\n===================\n", sents)

# Word tokenization
print("\nWord tokenization\n===================\n")
for index in range(len(sents)):
    words = tokenize.word_tokenize(sents[index])
    print(words)
```

</details>

<br>

2e. Write a program to find the most frequent noun tags.

<details>
<summary>CODE</summary>

```python
## 2e. Write a program to find the most frequent noun tags.
import nltk
from collections import defaultdict

nltk.download('averaged_perceptron_tagger')

text = nltk.word_tokenize("Ninad likes to play football. Ninad does not like to play cricket.")
tagged = nltk.pos_tag(text)
print(tagged)

# Checking if it is a noun or not
addNounWords = []
count = 0

for words in tagged:
    val = tagged[count][1]
    if val in ('NN', 'NNS', 'NNPS', 'NNP'):
        addNounWords.append(tagged[count][0])
    count += 1

print(addNounWords)

temp = defaultdict(int)

# Memoizing count
for sub in addNounWords:
    for wrd in sub.split():
        temp[wrd] += 1

# Getting max frequency
res = max(temp, key=temp.get)

# Printing result
print("Word with maximum frequency : " + str(res))
```

</details>

<br>

2f. Map words to the properties using Python Dictionaries.

<details>
<summary>CODE</summary>

```python
## 2f. Map Words to Properties Using Python Dictionaries

# Creating and printing a dictionary by mapping word with its properties
thisdict = {
    "brand": "Ford",
    "model": "Mustang",
    "year": 1964
}

print(thisdict)
print(thisdict["brand"])
print(len(thisdict))
print(type(thisdict))
```

</details>

<br>

2g. Study i) DefaultTagger, ii) Regular expression tagger, iii) UnigramTagger

<details>
<summary>CODE</summary>

```python
# i) DefaultTagger
import nltk
from nltk.tag import DefaultTagger
from nltk.corpus import treebank

# Create a default tagger that tags everything as 'NN'
exptagger = DefaultTagger('NN')

# Get test sentences from the treebank corpus
testsentences = treebank.tagged_sents()[1000:]

# Evaluate the tagger on the test sentences
print(exptagger.evaluate(testsentences))

## Tagging a list of sentences
print(exptagger.tag_sents([['Hi', ','], ['How', 'are', 'you', '?']]))


####

# ii) Regular expression tagger, 

from nltk.corpus import brown 
from nltk.tag import RegexpTagger 
test_sent = brown.sents(categories='news')[0] 
regexp_tagger = RegexpTagger( 
[(r'^-?[0-9]+(.[0-9]+)?$', 'CD'),   # cardinal numbers 
(r'(The|the|A|a|An|an)$', 'AT'),   # articles 
(r'.*able$', 'JJ'),                # adjectives 
(r'.*ness$', 'NN'),   # nouns formed from adjectives      
(r'.*ly$', 'RB'),     # adverbs        
(r'.*s$', 'NNS'),         # plural nouns   
(r'.*ing$', 'VBG'),   # gerunds              
(r'.*ed$', 'VBD'),  # past tense verbs 
(r'.*', 'NN')    # nouns (default)                    
]) 
print(regexp_tagger) 
print(regexp_tagger.tag(test_sent))

####

# iii) UnigramTagger

from nltk.tag import UnigramTagger
from nltk.corpus import treebank
train_sents = treebank.tagged_sents()[:10]
tagger = UnigramTagger(train_sents) # Initializing 
print(treebank.sents()[0])
print('\n',tagger.tag(treebank.sents()[0]))
tagger.tag(treebank.sents()[0])
tagger = UnigramTagger(model ={'Pierre': 'NN'}) #Overriding the context model print('\n',tagger.tag(treebank.sents()[0]))

```

</details>

<br>

2h. Find different words from a given plaintext without any spaces by comparing this text with a given corpus of words. Also find the score of words.

<details>
<summary>CODE</summary>

```python
## 2h. Find different words from a given plain text without any space by comparing this text with a given corpus of words. Also find the score of words.

from __future__ import with_statement  # with statement for reading file
import re  # Regular expression

words = []  # corpus file words
testword = []  # test words
ans = []  # words matches with corpus

print("MENU")
print("-----------")
print(" 1. Hash tag segmentation")
print(" 2. URL segmentation")
print("Enter the input choice for performing word segmentation:")
choice = int(input())

if choice == 1:
    text = "#whatismyname"  # hash tag test data to segment
    print("Input with HashTag:", text)
    pattern = re.compile("[^\w']")
    a = pattern.sub('', text)
elif choice == 2:
    text = "www.whatismyname.com"  # URL test data to segment
    print("Input with URL:", text)
    a = re.split('\s|(?<!\d)[,.](?!\d)', text)
    splitwords = ["www", "com", "in"]  # remove the words which is containing in the list
    a = "".join([each for each in a if each not in splitwords])
else:
    print("Wrong choice...try again")
    exit()

print(a)

for each in a:
    testword.append(each)  # test word
test_lenth = len(testword)  # length of the test data

# Reading the corpus
with open('words.txt', 'r') as f:
    lines = f.readlines()
    words = [e.strip() for e in lines]

def Seg(a, lenth):
    ans = []
    for k in range(0, lenth + 1):  # this loop checks char by char in the corpus
        if a[0:k] in words:
            print(a[0:k], "- appears in the corpus")
            ans.append(a[0:k])
            break
    if ans != []:
        g = max(ans, key=len)
        return g
    return ""

test_tot_itr = 0  # each iteration value
answer = []  # Store each word that contains the corpus
Score = 0  # initial value for score
N = 37  # total number of corpus
M = 0
C = 0

while test_tot_itr < test_lenth:
    ans_words = Seg(a, test_lenth)
    if ans_words != "":
        test_itr = len(ans_words)
        answer.append(ans_words)
        a = a[test_itr:test_lenth]
        test_tot_itr += test_itr

Aft_Seg = " ".join([each for each in answer])
# print segmented words in the list
print("Output")
print("---------")
print(Aft_Seg)  # print after segmentation the input

# Calculating Score
C = len(answer)
score = C * N / N  # Calculate the score
print("Score", score)
```

</details>

<br>




******************************************************

## Prac3

3a. Study of Wordnet Dictionary with methods as synsets, definitions, examples, antonyms.

<details>
<summary>CODE</summary>

```python
# NLP 3A. Study of Wordnet Dictionary with methods as synsets, definitions, examples, antonyms
# Import necessary libraries
import nltk
from nltk.corpus import wordnet

# Download WordNet (if not already downloaded)
nltk.download('wordnet')

# Get synsets (collection of synonyms) for "phone"
synsets = wordnet.synsets("phone")

# Print information about "phone"
print("**Word:** phone")
print("  * Synsets:")

# Loop through each synset and print its definition and examples
for synset in synsets:
    # Get the first word from the synset (considered the most representative)
    word = synset.lemmas()[0].name()
    print(f"      - Word: {word}")
    print(f"        - Definition: {synset.definition()}")
    print(f"          - Examples: {synset.examples()}")


print("-"*40)
# Get antonyms for "buy" (verb)
antonyms = wordnet.lemma('buy.v.01.buy').antonyms()

# Print antonyms for "buy"
print("\n**Antonyms for 'buy':")
for antonym in antonyms:
    print(f"  - {antonym.name()}")
```

</details>

<br>

3b. Study lemmas, hyponyms, hypernyms, entailments.

<details>
<summary>CODE</summary>

```python
# NLP 3B Study lemmas, hyponyms, hypernyms.

# Import necessary libraries
import nltk
from nltk.corpus import wordnet

# Download WordNet (if not already downloaded)
nltk.download('wordnet')

# Exploring Lemmas
print("\n**Lemmas**")
synsets = wordnet.synsets("computer")
print("  * Synsets and Lemmas:")
for synset in synsets:
    lemma_names = [lemma.name() for lemma in synset.lemmas()]
    print(f"      - Synset: {synset} --> Lemmas: {lemma_names}")

# Exploring Hyponyms
print("\n**Hyponyms**")
computer_synset = wordnet.synset("computer.n.01")
hyponyms = computer_synset.hyponyms()
print("  * Hyponyms of 'computer.n.01':")
for synset in hyponyms:
    lemma_names = [lemma.name() for lemma in synset.lemmas()]
    print(f"      - Synset: {synset} --> Lemmas: {lemma_names}")

# Exploring Hypernyms
print("\n**Hypernyms**")
vehicle_synset = wordnet.synset("vehicle.n.01")
car_synset = wordnet.synset("car.n.01")
lowest_common_hypernym = car_synset.lowest_common_hypernyms(vehicle_synset)
print(f"  * Lowest common hypernym of 'vehicle' and 'car': {lowest_common_hypernym[0]}")
```

</details>

<br>

3c. Write a program using python to find synonym and antonym of word "active" using Wordnet.

<details>
<summary>CODE</summary>

```python
# NLP 3C. Write a program using python to find synonym and antonym of word "active" using Wordnet.

import nltk
from nltk.corpus import wordnet
nltk.download('omw-1.4')

def get_synonyms_antonyms(word):
    synonyms = []
    antonyms = []

    for syn in wordnet.synsets(word):
        for lemma in syn.lemmas():
            synonyms.append(lemma.name())
            if lemma.antonyms():
                antonyms.append(lemma.antonyms()[0].name())

    return set(synonyms), set(antonyms)

def main():
    word = input("Enter the word:-")
    synonyms, antonyms = get_synonyms_antonyms(word)
    
    print("Synonyms of", word + ":", synonyms)
    print("Antonyms of", word + ":", antonyms)

if __name__ == "__main__":
    nltk.download('wordnet')
    main()
```

</details>

<br>

3d. Compare two nouns.

<details>
<summary>CODE</summary>

```python
#d. Compare two nouns.

from nltk.corpus import wordnet as wn

def compare_nouns(noun1, noun2):
    # Get synsets for each noun
    synsets1 = wn.synsets(noun1, pos=wn.NOUN)
    synsets2 = wn.synsets(noun2, pos=wn.NOUN)
    
    if not synsets1 or not synsets2:
        return "Unable to compare. Make sure both nouns are valid." 
    
    max_wup_similarity = 0
    max_path_similarity = 0
    
    for synset1 in synsets1:
        for synset2 in synsets2:
            # Calculate Wu-Palmer Similarity
            wup_similarity = synset1.wup_similarity(synset2)
            if wup_similarity is not None and wup_similarity > max_wup_similarity:
                max_wup_similarity = wup_similarity
            
            # Calculate Path Similarity
            path_similarity = synset1.path_similarity(synset2)
            if path_similarity is not None and path_similarity > max_path_similarity:
                max_path_similarity = path_similarity
    
    return max_wup_similarity, max_path_similarity

if __name__ == "__main__":
    noun1 = input("Enter the first noun: ")
    noun2 = input("Enter the second noun: ") 
    
    wup_similarity_score, path_similarity_score = compare_nouns(noun1, noun2)
    
    print(f"The Wu-Palmer Similarity between '{noun1}' and '{noun2}' is: {wup_similarity_score}")
    print(f"The Path Similarity between '{noun1}' and '{noun2}' is: {path_similarity_score}")
```

</details>

<br>

3e. Handling stopword

3e_i. Using nltk, add or remove stop words in NLTK's Default stop word list

<details>
<summary>CODE</summary>

```python
# NLP 3E_a: Using nltk, add or remove stop words in NLTK's Default stop word list
# Import necessary libraries
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

# Download stopwords (if not already downloaded)
nltk.download('punkt')
nltk.download('stopwords')

print("-"*40)

# Sample text
text = "Ninad likes to play Chess, however he is not too good with the football."
print("Given Text:- ",text)
# Remove stop words from text
text_tokens = word_tokenize(text)
stop_words = stopwords.words('english')
tokens_without_sw = [word for word in text_tokens if word not in stop_words]
print("Tokens without stop words:", tokens_without_sw)

# Add custom stop word ('not')
custom_stop_words = stop_words + ['not']
tokens_without_sw = [word for word in text_tokens if word not in custom_stop_words]
print("\nTokens without 'not' (custom stop word):", tokens_without_sw)
```

</details>

<br>

3e_ii. Using Gensim, add or remove stop words in Default Gensim stop words List.

<details>
<summary>CODE</summary>

```python
# NLP 3E_b: Using Gensim, add or remove stop words in Default Gensim stop words List.
#pip install scipy==1.12

import gensim
from gensim.parsing.preprocessing import remove_stopwords, STOPWORDS

text = "Ninad likes to play Chess, however he is not too good with the football."

# Removing Stop Words
filtered_sentence = remove_stopwords(text)
print("-"*30)
print("Original sentence:", text)
print("-"*30)
print("Stop words removed:", filtered_sentence)
print("-"*30)

# Adding Stop Words
all_stopwords = STOPWORDS.union(set(['likes', 'play']))
text_tokens = text.split()
tokens_without_sw = [word for word in text_tokens if word not in all_stopwords]
print("Original sentence (tokens):", text_tokens)
print("-"*30)
print("Stop words 'likes' and 'play' added:", tokens_without_sw)
print("-"*30)

# Removing Specific Stop Word
all_stopwords = STOPWORDS.difference({"not"})
tokens_without_sw = [word for word in text.split() if word not in all_stopwords]
print("Original sentence (tokens):", text.split())
print("-"*30)
print("Stop word 'not' removed:", tokens_without_sw)
print("-"*30)
```

</details>

<br>

3e_iii. Using SpaCy, add or remove Stop Words in Default SpaCy stop words List.

<details>
<summary>CODE</summary>

```python
# NLP 3E_c: Using SpaCy, add or remove Stop Words in Default SpaCy stop words List.

#python -m spacy download en_core_web_sm

import spacy
import nltk
from nltk.tokenize import word_tokenize
print("NLP 3E 3 Using Spacy Adding and Removing Stop Words in Default Spacy Stop Words List")
# Load spaCy model (assuming en_core_web_sm is already downloaded)
sp = spacy.load("en_core_web_sm")

# Get default stop words from spaCy
all_stopwords = sp.Defaults.stop_words

# Adding Stop Words

text = "Ninad likes to play Chess, however he is not too good with the football."
text_tokens = word_tokenize(text)

# Add "play" to stop words
all_stopwords.add("play")
tokens_without_sw = [word for word in text_tokens if word not in all_stopwords]
print("Original sentence (tokens):", text_tokens)
print("Stop word 'play' added:", tokens_without_sw)

# Removing Specific Stop Word

# Remove "not" from stop words (modify with caution)
all_stopwords.remove("not")
tokens_without_sw = [word for word in text_tokens if word not in all_stopwords]
print("Original sentence (tokens):", text_tokens)
print("Stop word 'not' removed:", tokens_without_sw)
```

</details>

<br>

******************************************************

## Prac4

4a. Tokenization using Python’s split() function.

<details>
<summary>CODE</summary>

```python
# 4A. Tokenization using Python’s split() function

# Sample text to tokenize
text = "Hello ! My name is Ninad Karlekar I live in mumbai"

# Tokenizing the text using split()
tokens = text.split()

# Printing the tokens
print("="*60)
print("4A. Tokenization using Python’s split() function")
print("-"*10)
print("Tokens:", tokens)
print("="*60)
```

</details>

<br>

4b. Tokenization using Regular Expression (RegEx).

<details>
<summary>CODE</summary>

```python
# 4b. Tokenization using Regular Expressions (RegEx)

import re

# Sample text to tokenize
text = "Hello ! My name is Ninad Karlekar I live in mumbai"

# Define the regex pattern for tokenization (splitting by whitespace)
pattern = r'\s+'

# Tokenizing the text using re.split()
tokens = re.split(pattern, text)

# Printing the tokens
print("="*60)
print("4b. Tokenization using Regular Expressions (RegEx)")
print("-"*10)
print("Tokens:", tokens)
print("="*60)
```

</details>

<br>

4c. Tokenization using NLTK.

<details>
<summary>CODE</summary>

```python
#4c. Tokenization using NLTK

import nltk
from nltk.tokenize import word_tokenize

nltk.download('punkt')

# Sample text to tokenize
text = "Hello ! My name is Ninad Karlekar I live in mumbai"

# Tokenizing the text using NLTK's word_tokenize()
tokens = word_tokenize(text)

# Printing the tokens
print("="*60)
print("4c. Tokenization using NLTK")
print("-"*10)
print("Tokens:", tokens)
print("="*60)
```

</details>

<br>

4d. Tokenization using spaCy library.

<details>
<summary>CODE</summary>

```python
#4d. Tokenization using the spaCy library

import spacy

# Load the English language model
nlp = spacy.blank("en")

# Text to be tokenized
text = "Hello ! My name is Ninad Karlekar I live in mumbai"

# Process the text with SpaCy
doc = nlp(text)

# Extract tokens
tokens = [token.text for token in doc]

# Print tokens
print("="*60)
print("4d. Tokenization using the spaCy library")
print("-"*10)
print("Tokens:", tokens)
print("="*60)
```

</details>

<br>

4e. Tokenization using Keras.

<details>
<summary>CODE</summary>

```python
#4e. Tokenization using Keras [COLAB]

import keras

from tensorflow.keras.preprocessing.text import text_to_word_sequence #works on colab

# Create a string input
str = "Hello ! My name is Ninad Karlekar I live in mumbai"

# tokenizing the text
tokens = text_to_word_sequence(str)
print("="*60)
print("4e. Tokenization using Keras")
print("-"*10)
print("Tokens:", tokens)
print("="*60)

####
# to run on local IDE(jupyter) use tenserflow version 2.13.0
# pip uninstall tensorflow     ## to uninstall latest version if installed
# pip install tensorflow==2.13.0
####

```

</details>

<br>

4f. Tokenization using Gensim.

<details>
<summary>CODE</summary>

```python
# 4f. Tokenization using Gensim
#pip install gensim
from gensim.utils import tokenize
# Create a string input
str = "Hello ! My name is Ninad Karlekar I live in mumbai"
# tokenizing the text
# Tokenizing the text
tokenized_words = list(tokenize(str))

# Printing each tokenized word separately
print("="*60)
print("4f. Tokenization using Gensim")
print("-"*10)
print("Tokens:", tokenized_words)
print("="*60)
```

</details>

<br>

******************************************************

## Prac5

5a. Word tokenization in Hindi

<details>
<summary>CODE</summary>

```python
## 5A. Word tokenization in Hindi

# cd "e:/GitHub/Practical_BscIT_MscIT_Ninad/MscIT/Semester 4/Natural_Language_Processing/Practical05/" for VSCODE terminal

# Clone the Indic NLP Library and Resources

# !git clone https://github.com/anoopkunchukuttan/indic_nlp_library.git
# !git clone https://github.com/anoopkunchukuttan/indic_nlp_resources.git

import sys
import os

#!pip install indic
# !pip install indic-nlp-library

# The path to the local git repo for Indic NLP library
INDIC_NLP_LIB_HOME = 'indic_nlp_library'

# The path to the local git repo for Indic NLP Resources
INDIC_NLP_RESOURCES = 'indic_nlp_resources'

# Add library to Python path
sys.path.append(os.path.join(INDIC_NLP_LIB_HOME, 'src'))

# Verify if the path was added correctly
print(sys.path)

# Set environment variable for resources folder
from indicnlp import common
common.set_resources_path(INDIC_NLP_RESOURCES)

from indicnlp.tokenize import indic_tokenize

indic_string = 'सुनो, कुछ आवाज़ आ रही है। फोन?'
print('Input String: {}'.format(indic_string))
print('Tokens: ')
for t in indic_tokenize.trivial_tokenize(indic_string):
    print(t)
```

</details>

<br>

5b. Generate similar sentences from a given Hindi text input

<details>
<summary>CODE</summary>

```python
## 5B. Generate similar sentences from a given Hindi text input.

synonyms = {
    "खुश": ["प्रसन्न", "आनंदित", "खुशी"],
    "बहुत": ["अधिक", "बहुत ज्यादा", "काफी"]
}
 
# Function to generate similar sentences by replacing some words with synonyms
def generate_similar_sentences(input_sentence, num_sentences=5):
    similar_sentences = []
 
    # Replace some words with synonyms 
    for word, word_synonyms in synonyms.items():
        for synonym in word_synonyms:
            modified_sentence = input_sentence.replace(word, synonym)
            similar_sentences.append(modified_sentence)
    return similar_sentences[:num_sentences]
 
input_sentence = "मैं आज बहुत खुश हूँ।"
similar_sentences = generate_similar_sentences(input_sentence)
print("Original sentence:", input_sentence)
print("Similar sentences:")
for sentence in similar_sentences:
    print("-", sentence)
```

</details>

<br>

5c. Identify the Indian language from the given text.

<details>
<summary>CODE</summary>

```python
## 5C. Identify the Indian language from the given text.

#pip install langid

import nltk
import langid
 
# Download necessary NLTK data
nltk.download('punkt')
 
def identify_language(text):
    lang, _ = langid.classify(text)
    return lang
 
# Identify the Indian Language from the given text
language = identify_language("नमस्ते, आप कैसे हैं?")
print("Identified language:", language)
```

</details>

<br>

******************************************************

## Prac6

6a. Part of speech Tagging and chunking of user defined text.

<details>
<summary>CODE</summary>

```python
# 6. Illustrate part of speech tagging.
## a) sentence tokenization, word tokenization, Part of speech Tagging and chunking of user defined text.

import nltk
from nltk import tokenize
from nltk import tag
from nltk import chunk

# Download necessary NLTK data
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
nltk.download('maxent_ne_chunker')
nltk.download('words')

# Paragraph to be tokenized
para = "Hello! My name is Ninad Karlekar. Today you'll be learning NLTK."

# Sentence tokenization
sents = tokenize.sent_tokenize(para)
print("\nsentence tokenization\n===================\n", sents)

# Word tokenization
print("\nword tokenization\n===================\n")
for index in range(len(sents)):
    words = tokenize.word_tokenize(sents[index])
    print(words)

# POS Tagging
tagged_words = []
for index in range(len(sents)):
    tagged_words.append(tag.pos_tag(words))
print("\nPOS Tagging\n===========\n", tagged_words)

# Chunking
tree = []
for index in range(len(sents)):
    tree.append(chunk.ne_chunk(tagged_words[index]))
print("\nchunking\n========\n", tree)
```

</details>

<br>

6b. Named Entity recognition of user defined text.

<details>
<summary>CODE</summary>

```python
# 6. Illustrate part of speech tagging.

## b) Named Entity recognition using user defined text.


# Install and download spaCy model
# !pip install -U spacy
# !python -m spacy download en_core_web_sm

import spacy

# Load English tokenizer, tagger, parser, and NER
nlp = spacy.load("en_core_web_sm")

# Process whole documents
text = (
    "When Sebastian Thrun started working on self-driving cars at "
    "Google in 2007, few people outside of the company took him "
    "seriously. “I can tell you very senior CEOs of major American "
    "car companies would shake my hand and turn away because I wasn’t "
    "worth talking to,” said Thrun, in an interview with Recode earlier "
    "this week."
)
doc = nlp(text)

# Analyse syntax
print("Noun phrases:", [chunk.text for chunk in doc.noun_chunks])
print("Verbs:", [token.lemma_ for token in doc if token.pos_ == "VERB"])
```

</details>

<br>

6c. Named Entity recognition with diagram using NLTK corpus - treebank

<details>
<summary>CODE</summary>

```python
# 6C: Named Entity recognition with diagram using NLTK corpus - treebank

import nltk
nltk.download('treebank')
from nltk.corpus import treebank_chunk
treebank_chunk.tagged_sents()[0]
treebank_chunk.chunked_sents()[0]
treebank_chunk.chunked_sents()[0].draw()


# Note: It runs on Python IDLE, VScode#
```

</details>

<br>

******************************************************

## Prac7

7a. Define grammar using nltk. Analyse a sentence using the same.

<details>
<summary>CODE</summary>

```python
# 7a. Define grammar using nltk. Analyse a sentence using the same.

import nltk
from nltk import tokenize

grammar1 = nltk.CFG.fromstring("""
    S -> VP
    VP -> VP NP
    NP -> Det NP
    Det -> 'that'
    NP -> singular Noun
    NP -> 'flight'
    VP -> 'Book'
""")

sentence = "Book that flight"
all_tokens = tokenize.word_tokenize(sentence)
print(all_tokens)

parser = nltk.ChartParser(grammar1)
for tree in parser.parse(all_tokens):
    print(tree)
    tree.draw()
```

</details>

<br>

7b. Accept the input string with Regular expression of FA: 101+

<details>
<summary>CODE</summary>

```python
# 7B. Accept the input string with Regular expression of FA: 101+

def FA(s):
    # If the length is less than 3, then it can't be accepted. Therefore, end the process.
    if len(s) < 3:
        return "Rejected"
    # The first three characters are fixed. Therefore, checking them using index.
    if s[0] == '1':
        if s[1] == '0':
            if s[2] == '1':
                # After index 2, only "1" can appear. Therefore, break the process if any other character is detected.
                for i in range(3, len(s)):
                    if s[i] != '1':
                        return "Rejected"
                return "Accepted"  # if all conditions are true
            return "Rejected"  # else of 3rd if
        return "Rejected"  # else of 2nd if
    return "Rejected"  # else of 1st if

inputs = ['1', '10101', '101', '10111', '01010', '100', '', '10111101', '1011111']
for i in inputs:
    print(FA(i))
```

</details>

<br>

7c. Accept the input string with Regular expression of FA: (a+b)*bba

<details>
<summary>CODE</summary>

```python
# 7C. Accept the input string with Regular expression of FA: (a+b)*bba

def FA(s):
    size = 0
    # Scan complete string and make sure that it contains only 'a' & 'b'
    for i in s:
        if i == 'a' or i == 'b':
            size += 1
        else:
            return "Rejected"
    # After checking that it contains only 'a' & 'b'
    # Check its length, it should be at least 3
    if size >= 3:
        # Check the last 3 elements
        if s[size - 3] == 'b':
            if s[size - 2] == 'b':
                if s[size - 1] == 'a':
                    return "Accepted"  # if all 4 if true
                return "Rejected"  # else of 4th if
            return "Rejected"  # else of 3rd if
        return "Rejected"  # else of 2nd if
    return "Rejected"  # else of 1st if

inputs = ['bba', 'ababbba', 'abba', 'abb', 'baba', 'bbb', '']
for i in inputs:
    print(FA(i))
```

</details>

<br>

7d. Implementation of Deductive Chart Parsing using context free grammar and a given sentence.

<details>
<summary>CODE</summary>

```python
# 7D. Implementation of Deductive Chart Parsing using context free grammar and a given sentence.

import nltk
from nltk import tokenize

# Define the context-free grammar (CFG)
grammar1 = nltk.CFG.fromstring("""
    S -> NP VP
    PP -> P NP
    NP -> Det N | Det N PP | 'I'
    VP -> V NP | VP PP
    Det -> 'a' | 'my'
    N -> 'bird' | 'balcony'
    V -> 'saw'
    P -> 'in'
""")

# Sentence to be tokenized and parsed
sentence = "I saw a bird in my balcony"
all_tokens = tokenize.word_tokenize(sentence)
print(all_tokens)
# all_tokens = ['I', 'saw', 'a', 'bird', 'in', 'my', 'balcony']

# Create a parser using the defined grammar
parser = nltk.ChartParser(grammar1)

# Parse the tokenized sentence and print the parse trees
for tree in parser.parse(all_tokens):
    print(tree)
    tree.draw()
```

</details>

<br>

******************************************************

## Prac8

8a. Study PorterStemmer, LancasterStemmer, RegexpStemmer, SnowballStemmer

<details>
<summary>CODE</summary>

```python
# PorterStemmer
import nltk
from nltk.stem import PorterStemmer
word_stemmer = PorterStemmer()
print("Output of PorterStemmer:-")
print(word_stemmer.stem('Ninad is running'))
print("*"*50)

# LancasterStemmer
import nltk
from nltk.stem import LancasterStemmer
Lanc_stemmer = LancasterStemmer()
print("Output of LancasterStemmer:-")
print(Lanc_stemmer.stem('jumping'))
print("*"*50)

#RegexpStemmer
import nltk
from nltk.stem import RegexpStemmer
Reg_stemmer = RegexpStemmer('ing$|s$|e$|able$', min=4)
print("Output of RegexpStemmer:-")
print(Reg_stemmer.stem('writing'))
print("*"*50)

# SnowballStemmer
import nltk
from nltk.stem import SnowballStemmer
english_stemmer = SnowballStemmer('english')
print("Output of SnowballStemmer:-")
print(english_stemmer.stem ('writing'))
print("*"*50)
```

</details>

<br>

8b. Study WordNet Lemmatizer

<details>
<summary>CODE</summary>

```python
# WordNetLemmatizer
print("Output of WordNetLemmatizer:-")
import nltk
nltk.download('wordnet')

from nltk.stem import WordNetLemmatizer

lemmatizer = WordNetLemmatizer()
print("word :\tlemma")
print("rocks :", lemmatizer.lemmatize("books"))
print("corpora :", lemmatizer.lemmatize("corpora"))

# a denotes adjective in "pos"
print("worse :", lemmatizer.lemmatize("worse", pos ="a"))
print("*"*50)
```

</details>

<br>

******************************************************

## Prac9

9. Implement Naive Bayes classifier


<details>
<summary>CODE</summary>

```python
# 9. Implement Naive Bayes classifier

# pip install pandas
# pip install sklearn
import pandas as pd
import numpy as np
import re
import nltk
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

# Download NLTK stopwords
nltk.download('stopwords')

# Read the SMS data
sms_data = pd.read_csv("MscIT\\Semester 4\\Natural_Language_Processing\\Practical09\\spam.csv", encoding='latin-1')

# Initialize the Porter Stemmer
stemming = PorterStemmer()
corpus = []

# Create the corpus with stemmed words
for i in range(len(sms_data)):
    s1 = re.sub('[^a-zA-Z]', ' ', sms_data['v2'][i])
    s1 = s1.lower()
    s1 = s1.split()
    s1 = [stemming.stem(word) for word in s1 if word not in set(stopwords.words('english'))]
    s1 = ' '.join(s1)
    corpus.append(s1)

# Convert the corpus into a matrix of token counts
countvectorizer = CountVectorizer()
x = countvectorizer.fit_transform(corpus).toarray()
print(x)

# Extract the target variable
y = sms_data['v1'].values
print(y)

# Split the data into training and testing sets
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, stratify=y, random_state=2)

# Initialize the Multinomial Naïve Bayes model
multinomialnb = MultinomialNB()
multinomialnb.fit(x_train, y_train)

# Predicting on test data
y_pred = multinomialnb.predict(x_test)
print(y_pred)

# Results of our models
print(classification_report(y_test, y_pred))
print("accuracy_score: ", accuracy_score(y_test, y_pred))

```

</details>

<br>

******************************************************

## Prac10

10a. Speech Tagging:

10a_i. Speech tagging using spacy.

<details>
<summary>CODE</summary>

```python
# 10. Speech Tagging
## a_1. Speech tagging using spacy

import spacy

sp = spacy.load('en_core_web_sm')
sen = sp(u"I like to play Table tennis. I hated it in my childhood though")

print(sen.text)
print(sen[7].pos_)
print(sen[7].tag_)
print(spacy.explain(sen[7].tag_))

for word in sen:
    print(f'{word.text:{12}} {word.pos_:{10}} {word.tag_:{8}} {spacy.explain(word.tag_)}')

sen = sp(u'Can you google it?')
word = sen[2]
print(f'{word.text:{12}} {word.pos_:{10}} {word.tag_:{8}} {spacy.explain(word.tag_)}')

sen = sp(u'Can you search it on google?')
word = sen[5]
print(f'{word.text:{12}} {word.pos_:{10}} {word.tag_:{8}} {spacy.explain(word.tag_)}')

# Finding the Number of POS Tags
sen = sp(u"I like to play Table tennis. I hated it in my childhood though")
num_pos = sen.count_by(spacy.attrs.POS)
num_pos

for k,v in sorted(num_pos.items()):
    print(f'{k}. {sen.vocab[k].text:{8}}: {v}')

# Visualizing Parts of Speech Tags
from spacy import displacy

sen = sp(u"I like to play Table tennis. I hated it in my childhood though")
displacy.serve(sen, style='dep', options={'distance': 120}, port=8000)

# Visit http://127.0.0.1:8000/
```

</details>

<br>

10a_ii. Speech tagging using NLTK.

<details>
<summary>CODE</summary>

```python
## 10A_2. speech tagging using nktl

import nltk
nltk.download('state_union')
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')

from nltk.corpus import state_union
from nltk.tokenize import PunktSentenceTokenizer

# Create our training and testing data:
train_text = state_union.raw("E:\\GitHub\\Practical_BscIT_MscIT_Ninad\\MscIT\\Semester 4\\Natural_Language_Processing\\Practical10\\2005-GWBush.txt")
sample_text = state_union.raw("E:\\GitHub\\Practical_BscIT_MscIT_Ninad\\MscIT\\Semester 4\\Natural_Language_Processing\\Practical10\\2006-GWBush.txt")

# Train the Punkt tokenizer:
custom_sent_tokenizer = PunktSentenceTokenizer(train_text)

# Tokenize:
tokenized = custom_sent_tokenizer.tokenize(sample_text)

def process_content():
    try:
        for i in tokenized[:2]:
            words = nltk.word_tokenize(i)
            tagged = nltk.pos_tag(words)
            print(tagged)
    except Exception as e:
        print(str(e))

process_content()
```

</details>

<br>

10b. Statistical parsing:

10b_i. Usage of Give and Gave in the Penn Treebank sample.

<details>
<summary>CODE</summary>

```python
# 10 b. Statistical parsing:
# 1. Usage of Give and Gave in the Penn Treebank sample.

import nltk
import nltk.parse.viterbi
import nltk.parse.pchart

def give(t):
    return (t.label() == 'VP' and len(t) > 2 and t[1].label() == 'NP' and 
            (t[2].label() == 'PP-DTV' or t[2].label() == 'NP') and 
            ('give' in t[0].leaves() or 'gave' in t[0].leaves()))

def sent(t):
    return ' '.join(token for token in t.leaves() if token[0] not in '*-0')

def print_node(t, width):
    output = "%s %s: %s / %s: %s" % (
        sent(t[0]), t[1].label(), sent(t[1]), t[2].label(), sent(t[2]))
    if len(output) > width:
        output = output[:width] + "..."
    print(output)

for tree in nltk.corpus.treebank.parsed_sents():
    for t in tree.subtrees(give):
        print_node(t, 72)
```

</details>

<br>

10b_ii. Probabilistic parser

<details>
<summary>CODE</summary>

```python
# 10 b. Statistical parsing:
# 2. probabilistic parser

import nltk
from nltk import PCFG

grammar = PCFG.fromstring('''
    NP -> NNS [0.5] | JJ NNS [0.3] | NP CC NP [0.2]
    NNS -> "men" [0.1] | "women" [0.2] | "children" [0.3] | NNS CC NNS [0.4]
    JJ -> "old" [0.4] | "young" [0.6]
    CC -> "and" [0.9] | "or" [0.1]
''')

print(grammar)

viterbi_parser = nltk.ViterbiParser(grammar)
token = "old men and women".split()
obj = viterbi_parser.parse(token)

print("Output:")
for x in obj:
    print(x)
```

</details>

<br>

10c. Malt parsing:

10c_i. Parse a sentence and draw a tree using malt parsing

<details>
<summary>CODE</summary>

```python
# c. Malt parsing: i. Parse a sentence and draw a tree using malt parsing

# Copy maltparser-1.7.2 (unzipped version) and engmalt.linear-1.7.mco files to
# C:\Users\Beena Kapadia\AppData\Local\Programs\Python\Python39 folder
# Java should be installed
# Environment variables should be set:
# MALT_PARSER - C:\Users\Beena Kapadia\AppData\Local\Programs\Python\Python39\maltparser-1.7.2
# MALT_MODEL - C:\Users\Beena Kapadia\AppData\Local\Programs\Python\Python39\engmalt.linear-1.7.mco

from nltk.parse import malt

mp = malt.MaltParser('maltparser-1.7.2', 'engmalt.linear-1.7.mco')
t = mp.parse_one('I saw a bird from my window.'.split()).tree()
print(t)
t.draw()
```

</details>

<br>

******************************************************

## Prac11

11a. Multiword Expressions in NLP

<details>
<summary>CODE</summary>

```python
# 11 a. Multiword Expressions in NLP

# Multiword Expressions in NLP
from nltk.tokenize import MWETokenizer
from nltk import sent_tokenize, word_tokenize

s = '''Good cake cost Rs.1500\kg in Mumbai. Please buy me one of them.\n\nThanks.'''
mwe = MWETokenizer([('New', 'York'), ('Hong', 'Kong')], separator='_')

for sent in sent_tokenize(s):
    print(mwe.tokenize(word_tokenize(sent)))
```

</details>

<br>

11b. Normalized Web Distance and Word Similarity

<details>
<summary>CODE</summary>

```python
# 11 B. Normalized Web Distance and Word Similarity

# Convert # Reliance supermarket # Reliance hypermarket # Reliance # Reliance # Reliance downtown # Relianc market # Mumbai # Mumbai Hyper # Mumbai dxb # mumbai airport
# k.m trading # KM Trading # KM trade # K.M. Trading # KM.Trading # into # Reliance # Reliance # Reliance # Reliance # Reliance # Reliance # Mumbai # Mumbai # Mumbai
# Mumbai # KM Trading # KM Trading # KM Trading # KM Trading # KM Trading

import numpy as np
import re
import textdistance  # pip install textdistance
# we will need scikit-learn>=0.21
import sklearn  # pip install sklearn
from sklearn.cluster import AgglomerativeClustering

texts = [
    'Reliance supermarket', 'Reliance hypermarket', 'Reliance', 'Reliance', 
    'Reliance downtown', 'Reliance market', 'Mumbai', 'Mumbai Hyper', 
    'Mumbai dxb', 'mumbai airport', 'k.m trading', 'KM Trading', 
    'KM trade', 'K.M. Trading', 'KM.Trading'
]

def normalize(text):
    """ Keep only lower-cased text and numbers"""
    return re.sub('[^a-z0-9]+', ' ', text.lower())

def group_texts(texts, threshold=0.4):
    """ Replace each text with the representative of its cluster"""
    normalized_texts = np.array([normalize(text) for text in texts])
    distances = 1 - np.array([
        [textdistance.jaro_winkler(one, another) for one in normalized_texts]
        for another in normalized_texts
    ])
    clustering = AgglomerativeClustering(
        distance_threshold=threshold,  # this parameter needs to be tuned carefully
        affinity="precomputed", linkage="complete", n_clusters=None
    ).fit(distances)

    # clustering = AgglomerativeClustering(
    #     distance_threshold=threshold,  # this parameter needs to be tuned carefully
    #     metric="precomputed", linkage="complete", n_clusters=None
    # ).fit(distances)   ## Use this if above line is not working
    
    centers = dict()
    for cluster_id in set(clustering.labels_):
        index = clustering.labels_ == cluster_id
        centrality = distances[:, index][index].sum(axis=1)
        centers[cluster_id] = normalized_texts[index][centrality.argmin()]
    
    return [centers[i] for i in clustering.labels_]

print(group_texts(texts))
```

</details>

<br>

11c. Word Sense Disambiguation

<details>
<summary>CODE</summary>

```python
# 11C. Word Sense Disambiguation
from nltk.corpus import wordnet as wn

def get_first_sense(word, pos=None):
    if pos:
        synsets = wn.synsets(word, pos)
    else:
        synsets = wn.synsets(word)
    return synsets[0]

best_synset = get_first_sense('bank')
print('%s: %s' % (best_synset.name(), best_synset.definition()))

best_synset = get_first_sense('set', 'n')
print('%s: %s' % (best_synset.name(), best_synset.definition()))

best_synset = get_first_sense('set', 'v')
print('%s: %s' % (best_synset.name(), best_synset.definition()))

```

</details>

<br> 

******************************************************





